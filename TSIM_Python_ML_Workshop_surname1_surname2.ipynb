{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benedetta-c/workshop_tsim/blob/main/TSIM_Python_ML_Workshop_surname1_surname2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvMNe0-LOE8S"
      },
      "source": [
        "# ***Advanced Methods in Medical Signals and Images: Python & ML Workshop***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ahg6phv4Oz9A"
      },
      "source": [
        "Welcome to the Python & ML Workshop of the subject, where you will learn how to perform some data science tasks in Python by using Google Colab and multiple commonly-used libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGz1hyQg3Fhz"
      },
      "source": [
        "To get started, follow the following steps:\n",
        "\n",
        "1.   Sign in with your Google account (upper left button)\n",
        "1.   Click in *File/Save a copy in Drive*\n",
        "2.   Change the name of the norbook by double clicking in the name of the file (upper right field), with the format TSIM_Python_ML_Workshop_surname1_surname2.ipynb\n",
        "3. Click on *Connect* (upper left corner)\n",
        "4. Now you can start editing the notebook! The modified notebook will be saved in your Drive account. When you are finished, click on *File/Download .ipynb* to export the file and upload it to Moodle.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efXT5Tcz1mku"
      },
      "source": [
        "Double click here to edit your name and surname\n",
        "\n",
        "**NAME AND SURNAME:**\n",
        "\n",
        "**NAME AND SURNAME:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Advanced Methods in Medical Signals and Images: Python & ML Workshop](#scrollTo=kvMNe0-LOE8S)\n",
        "\n",
        ">[PART 1 - Exploring Survival on Titanic](#scrollTo=GDt0sTITEO3y)\n",
        "\n",
        ">>[1.1 - Load dataset](#scrollTo=8TpbPOtrGFm5)\n",
        "\n",
        ">>[1.2 - Preview dataset](#scrollTo=e7m5MqP1HhI3)\n",
        "\n",
        ">>>[1.2.1 - Generate descriptive statistics](#scrollTo=pcLKNDYxH0EA)\n",
        "\n",
        ">>>[1.2.2 - Show null columns](#scrollTo=zOO7AtBcH_Af)\n",
        "\n",
        ">>>[1.2.3 - Show missing values](#scrollTo=atq79PlDIGdg)\n",
        "\n",
        ">>>[1.2.4 - Visualize the target variable](#scrollTo=62tvtD7RIdDP)\n",
        "\n",
        ">>>[1.2.5 - Survival by age](#scrollTo=q2ffHEzDI3mv)\n",
        "\n",
        ">>>[1.2.6 - Survival by gender](#scrollTo=uRZYWa1SJAz_)\n",
        "\n",
        ">>>[1.2.7 - Survivers by class](#scrollTo=2BOrU0_CJKXH)\n",
        "\n",
        ">>>[1.2.8 - Survivers by port of embarkation](#scrollTo=0o8DJpIrJi-3)\n",
        "\n",
        ">>[1.3 - Initiate independent and dependent variables](#scrollTo=uYthnGhRJwC3)\n",
        "\n",
        ">>[1.4 - Feature engineering](#scrollTo=iKxL9XUkLUoe)\n",
        "\n",
        ">>[1.5 - Preprocess data with Scikit-Learn](#scrollTo=ZlcT_2sYLrfG)\n",
        "\n",
        ">>>[1.5.1 - Remove features that we don't want](#scrollTo=m73cqs6TL6xf)\n",
        "\n",
        ">>>[1.5.2 - Fill missing values (imputation)](#scrollTo=4MNGqyx-MCcr)\n",
        "\n",
        ">>>[1.5.3 - Handle categorical data](#scrollTo=C7oOTsHfMCgq)\n",
        "\n",
        ">>>[1.5.4 - Scale numeric data](#scrollTo=hNM6IsTKMCkm)\n",
        "\n",
        ">>[1.6 - Split into Training and Testing Sets](#scrollTo=fCjR5C2pOUdl)\n",
        "\n",
        ">>[1.7 - Classification task](#scrollTo=Htes449IPIxN)\n",
        "\n",
        ">>>[1.7.1 - Create classifiers](#scrollTo=qIX_n0_JQz80)\n",
        "\n",
        ">>>[1.7.2 - Train classifiers](#scrollTo=IA-KA9ECQ2jl)\n",
        "\n",
        ">>>[1.7.3 - Measure performance of the model](#scrollTo=Ck8VOySeR7xr)\n",
        "\n",
        ">[PART 2 - A quick glance at manual feature extraction](#scrollTo=Bp1EsomzEq0x)\n",
        "\n",
        ">>[2.1 - Load images](#scrollTo=skn-WikqWxVx)\n",
        "\n",
        ">>[2.2 - Preprocess images](#scrollTo=e425sDr7W1K5)\n",
        "\n",
        ">>[2.3 - Visualize images](#scrollTo=j5YE_zaXY44o)\n",
        "\n",
        ">>[2.4 - Manual extraction of features](#scrollTo=uhMAdCcsZfbv)\n",
        "\n",
        ">>[2.5 - Visualize features extracted](#scrollTo=QQOAGoJ4cJ19)\n",
        "\n",
        ">[PART 3 - Playing aroung with a Multilayer Perceptron (MLP) and training parameters](#scrollTo=3KO0Q0XxkAPo)\n",
        "\n",
        ">[[RECOMMENDED] Useful Resources](#scrollTo=ffL-F9Eekfzu)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "VQ9odFwXA_Nz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E0Nv78xa_jCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are some packages we are going to use in this workshop:"
      ],
      "metadata": {
        "id": "CigfDlgaE73p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import fetch_openml\n",
        "import cv2"
      ],
      "metadata": {
        "id": "XTQgbTgjFDeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 1 - Exploring Survival on Titanic"
      ],
      "metadata": {
        "id": "GDt0sTITEO3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 - Load dataset"
      ],
      "metadata": {
        "id": "8TpbPOtrGFm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are going to load the Titanic dataset and store them in a Pandas Dataframe (see more info about DataFrames [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)), which we will call `df`. `df` has both the input information and the ground truth label of what we want to estimate, the survival (column `target`)."
      ],
      "metadata": {
        "id": "qMUblGBCGnzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "titanic = fetch_openml('titanic', version=1, as_frame=True)\n",
        "df = titanic['data']\n",
        "df['survived'] = titanic['target']"
      ],
      "metadata": {
        "id": "i7JDwWwzEpoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 - Preview dataset"
      ],
      "metadata": {
        "id": "e7m5MqP1HhI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by undestanding our dataset. In order to print the three first entries of our dataframe, we will use the `head` method. If we only put `df` (the DataFrame itself), we will print as many entries as we can via `stdout` (standard output), which sometimes may be messy due to all the data we have."
      ],
      "metadata": {
        "id": "HUZPuO9mHk33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "8R7PGiwfHaFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "7x9Jtzc0H6Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.1 - Generate descriptive statistics"
      ],
      "metadata": {
        "id": "pcLKNDYxH0EA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "LIg50jG_HwV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 - Show null columns"
      ],
      "metadata": {
        "id": "zOO7AtBcH_Af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "5cAcEMxJHuEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.3 - Show missing values"
      ],
      "metadata": {
        "id": "atq79PlDIGdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "lCcHbd2lILpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can observe, a lot of columns are missing values. Let's visualize this better:"
      ],
      "metadata": {
        "id": "w5W97GwqIQtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "miss_vals = pd.DataFrame(df.isnull().sum() / len(df) * 100)\n",
        "miss_vals.plot(kind='bar',\n",
        "    title='Missing values in percentage',\n",
        "    ylabel='percentage'\n",
        "    )\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7OYXbEAhIQRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.4 - Visualize the target variable"
      ],
      "metadata": {
        "id": "62tvtD7RIdDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.survived.value_counts().plot(kind='bar')\n",
        "\n",
        "plt.xlabel('Survival')\n",
        "plt.ylabel('# of passengers')\n",
        "plt.title('Number of passengers based on their survival')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AaOaKFOpIkbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.5 - Survival by age"
      ],
      "metadata": {
        "id": "q2ffHEzDI3mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.hist(df['age'][df.survived == '0'].dropna(), label='Not survived')\n",
        "ax.hist(df['age'][df.survived == '1'].dropna(), label='Survived')\n",
        "\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Survival count')\n",
        "plt.title('Survivals by age')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HoGDMDGOI7aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.6 - Survival by gender"
      ],
      "metadata": {
        "id": "uRZYWa1SJAz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['survived'] = df.survived.astype('int')\n",
        "\n",
        "sns.barplot(\n",
        "    x='sex',\n",
        "    y='survived',\n",
        "    data=df\n",
        ")\n",
        "\n",
        "plt.title('Survival by gender')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xJv-p5RsJDHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.7 - Survivers by class"
      ],
      "metadata": {
        "id": "2BOrU0_CJKXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize survivers by class wither by counting them (absolute) or by using a percentage of the total (relative)"
      ],
      "metadata": {
        "id": "cPFA9m_dJQTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='pclass', data=df)\n",
        "plt.title('Unique survivers by class (absolute)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JS4P67mvJMmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x='pclass', y='survived', data=df)\n",
        "plt.title('Percent survivers by class (relative)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iS0q3yDgJaPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.8 - Survivers by port of embarkation"
      ],
      "metadata": {
        "id": "0o8DJpIrJi-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x='embarked', y='survived', data=df)\n",
        "plt.title('Percent survivers by port of embarkation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wu2DaRr0JnvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 - Initiate independent and dependent variables"
      ],
      "metadata": {
        "id": "uYthnGhRJwC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification algorithms will try to classify targets (dependent variables) using the features (independent variables) as predictors.\n",
        "\n",
        "Here, the column that we want to make a prediction on is the column that states whether or not the passenger survived (column name = `target`).\n",
        "\n",
        "We can assign the features to **`X`** by using the `drop` method to keep all columns except the target and assigning the target to **`y`**."
      ],
      "metadata": {
        "id": "PqVYZiTkKGrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign Dependent and Independent variables\n",
        "X = df.drop('survived', axis=1)\n",
        "y = df['survived']"
      ],
      "metadata": {
        "id": "2Szb-82rKckP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 - Feature engineering"
      ],
      "metadata": {
        "id": "iKxL9XUkLUoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some features don’t have much meaning when used alone. However, we can give them meaning by looking at the context.\n",
        "\n",
        "For example, the `sibsp` and `parch` columns tell you if the passenger was travelling with siblings, parents or children. By combining these features, you can infer if the passenger was travelling alone, and see if that impacted the chances of survival."
      ],
      "metadata": {
        "id": "WWIBCgfBLcJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X['family'] = X['sibsp'] + X['parch']\n",
        "X.loc[X['family'] > 0, 'travelled_alone'] = 0\n",
        "X.loc[X['family'] == 0, 'travelled_alone'] = 1\n",
        "X.drop(['family', 'sibsp', 'parch'], axis=1, inplace=True)\n",
        "sns.countplot(x='travelled_alone', data=X)\n",
        "plt.title('Number of passengers travelling alone')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vst6a06NLbze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 - Preprocess data with Scikit-Learn"
      ],
      "metadata": {
        "id": "ZlcT_2sYLrfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will use Scikit-Learn. For further information, [here](https://scikit-learn.org/stable/) is their official webpage."
      ],
      "metadata": {
        "id": "3a1473n5MtCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two main reasons why you want to do data preprocessing before training your machine learning model:\n",
        "\n",
        "- To satisfy the requirements of the scikit-learn api\n",
        "- To clean erroneous and missing data from datasets\n",
        "\n",
        "Hence, we will:\n",
        "\n",
        "- Remove features that we don’t want\n",
        "- Fill missing values\n",
        "- Convert categorical data features to numeric format\n",
        "- Scale numeric features"
      ],
      "metadata": {
        "id": "_TNVjr9ILzdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5.1 - Remove features that we don't want"
      ],
      "metadata": {
        "id": "m73cqs6TL6xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Complete the following lines of code marked with **TODO** (To-Do) in order to remove those features we don't want"
      ],
      "metadata": {
        "id": "zJ_N9wzrM2b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: remove high missing value columns: 'cabin', 'boat' and 'body'\n",
        "X.drop([...], axis=1, inplace=True)\n",
        "\n",
        "# TODO: remove less interesting features: 'name', 'ticket' and 'home.dest'\n",
        "X.drop([...], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "nzVdMXHfMWVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5.2 - Fill missing values (imputation)"
      ],
      "metadata": {
        "id": "4MNGqyx-MCcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use Scikit-learn, you should have no missing values in your dataset.\n",
        "\n",
        "Thus, using `SimpleImputer`, we will fill the missing values using the `mean` for numeric data, and the `most_frequent` value for categorical data."
      ],
      "metadata": {
        "id": "uyevOJstMl2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "def get_parameters(df):\n",
        "    parameters = {}\n",
        "    for col in df.columns[df.isnull().any()]:\n",
        "        if df[col].dtype == 'float64' or df[col].dtype == 'int64' or df[col].dtype =='int32':\n",
        "            strategy = 'mean'\n",
        "        else:\n",
        "            strategy = 'most_frequent'\n",
        "        missing_values = df[col][df[col].isnull()].values[0]\n",
        "        parameters[col] = {'missing_values':missing_values, 'strategy':strategy}\n",
        "    return parameters\n",
        "\n",
        "parameters = get_parameters(X)\n",
        "\n",
        "for col, param in parameters.items():\n",
        "    missing_values = param['missing_values']\n",
        "    strategy = param['strategy']\n",
        "    imp = SimpleImputer(missing_values=missing_values, strategy=strategy)\n",
        "    X[col] = imp.fit_transform(X[[col]])\n",
        "\n",
        "X.isnull().sum()"
      ],
      "metadata": {
        "id": "VeKzArNTNKYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5.3 - Handle categorical data"
      ],
      "metadata": {
        "id": "C7oOTsHfMCgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit learn requires `categorical` data to be converted into continuous numeric format. Using the `pandas` `get_dummies` method, we will convert categorical features into 0s and 1s."
      ],
      "metadata": {
        "id": "eKSKWJEYNRre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# handle categorical data\n",
        "cat_cols = X.select_dtypes(include=['object','category']).columns\n",
        "dummies = pd.get_dummies(X[cat_cols], drop_first=True)\n",
        "X[dummies.columns] = dummies\n",
        "X.drop(cat_cols, axis=1, inplace=True)\n",
        "X.head()"
      ],
      "metadata": {
        "id": "aODJawwENQ_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5.4 - Scale numeric data"
      ],
      "metadata": {
        "id": "hNM6IsTKMCkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve model performance, we will scale the numeric features so that they all have a `mean=0` and a `standard deviation = 1`."
      ],
      "metadata": {
        "id": "5N3rsbJdNedW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Complete the following lines of code marked with **TODO**."
      ],
      "metadata": {
        "id": "Oo3pt6ViN9XN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale numeric data.\n",
        "# TODO: Import the package StandardScaler from sklearn.preprocessing\n",
        "\n",
        "\n",
        "# Select numerical columns\n",
        "num_cols = X.select_dtypes(include=['int64', 'float64', 'int32']).columns\n",
        "\n",
        "# Apply StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X[num_cols] = scaler.fit_transform(X[num_cols])"
      ],
      "metadata": {
        "id": "qF829vETNjQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 - Split into Training and Testing Sets"
      ],
      "metadata": {
        "id": "fCjR5C2pOUdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could apply the machine learning model to the entire dataset, but that wouldn’t be useful to evaluate the model performance. Instead, we split the dataset into **training** and **testing** datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "__HrtvPNOdvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: import train_test_split from sklearn.model_selection. HINT: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "\n",
        "\n",
        "# TODO: We define a random state (seed) equal to 42 (could be other number, but we put 42 for the sake of reproducibility)\n",
        "RAND_STATE =\n",
        "\n",
        "# TODO: Split the whole dataset intro training and testing subsets. Use a test_size of 30% and a random_state equal to RAND_STATE, defined above.\n",
        "X_train, X_test, y_train, y_test ="
      ],
      "metadata": {
        "id": "4TSbsLMbOlXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.7 - Classification task"
      ],
      "metadata": {
        "id": "Htes449IPIxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7.1 - Create classifiers"
      ],
      "metadata": {
        "id": "qIX_n0_JQz80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, you must implement some classifiers. Here are some suggestions:\n",
        "1.   K-NN (HINT/ [sklearn - KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier))\n",
        "2.   SVM. You can try different kernels. (HINT/ [sklearn - SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html))\n",
        "3.   MLP (HINT/ [sklearn - MLP](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier))\n",
        "\n",
        "**Note I:** Do not forget to import the required libraries.\n",
        "\n",
        "**Note II:** Implement as many classifiers as you want to practice, that's why we indicate the last classifier with \"N\". Implement at least 3 classifiers.\n",
        "\n"
      ],
      "metadata": {
        "id": "NAZ5ZeOBPa3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cl1 =  # TODO\n",
        "cl2 =  # TODO\n",
        "clN =  # TODO"
      ],
      "metadata": {
        "id": "0rDo7z8uPuPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7.2 - Train classifiers"
      ],
      "metadata": {
        "id": "IA-KA9ECQ2jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Train (fit) each classifier with default configurations. Hint: see the \"fit\" method in each of the sklearn classifier documentations.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ViUXaDT0Q6hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7.3 - Measure performance of the model"
      ],
      "metadata": {
        "id": "Ck8VOySeR7xr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to measure the performance of the metric, let's predict with our model our **test** set"
      ],
      "metadata": {
        "id": "szhv8-fFSJ77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "# Predict\n",
        "yhat_cl1_test = cl1.predict(X_test)\n",
        "yhat_cl2_test = cl2.predict(X_test)\n",
        "yhat_clN_test = clN.predict(X_test)\n",
        "# Compute metrics (accuracy)\n",
        "acc_cl1 = accuracy_score(y_test, yhat_cl1_test)\n",
        "acc_cl2 = accuracy_score(y_test, yhat_cl2_test)\n",
        "acc_clN = accuracy_score(y_test, yhat_clN_test)"
      ],
      "metadata": {
        "id": "gp_MwRKoR7Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test ACC cl1:', acc_cl1)\n",
        "print('Test ACC cl2:', acc_cl2)\n",
        "print('Test ACC clN:', acc_clN)"
      ],
      "metadata": {
        "id": "vHxQqogxTQmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model with the best accuracy score is the one who better performed on the **test** set, yielding a better generalization to new data. The process is stochastic, so each time we train a model, we can obtain different results."
      ],
      "metadata": {
        "id": "qu31oN8UTyrD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 2 - A quick glance at manual feature extraction"
      ],
      "metadata": {
        "id": "Bp1EsomzEq0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this other part of the workshop, we are going to manually extract some features from some sample medical images."
      ],
      "metadata": {
        "id": "dM5xPF9LUjyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 - Load images"
      ],
      "metadata": {
        "id": "skn-WikqWxVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, upload the `.png` and .jpg files downloaded from Moodle to Google Colab (in the left menu, click on *Files*/*Upload* to upload the files)"
      ],
      "metadata": {
        "id": "vnFrJjAD5psN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Complete the following lines of code marked with **TODO**."
      ],
      "metadata": {
        "id": "C1QrzIhRDHbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: import imread from skimage.io\n",
        "\n",
        "\n",
        "# Read images\n",
        "img1 = imread('angio-mri.png')\n",
        "img2 = imread('cxr.png')\n",
        "img3 = imread('fundus.jpg')"
      ],
      "metadata": {
        "id": "JgKf8GBJExOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 - Preprocess images"
      ],
      "metadata": {
        "id": "e425sDr7W1K5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are going to preprocess the images so that our input images are 256x256 (grayscale)"
      ],
      "metadata": {
        "id": "kF6yjyV2YzRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from skimage.transform import resize\n",
        "from skimage.color import rgb2gray\n",
        "\n",
        "def preprocess_img(img, fundus=False, angio=False):\n",
        "  if fundus:\n",
        "    img = rgb2gray(img)\n",
        "  elif angio:\n",
        "    img = img[:,:,0]\n",
        "  img = resize(img, (256,256))  # Resize images to 256x256\n",
        "  img = cv2.normalize(img,None,0,255,cv2.NORM_MINMAX).astype(np.uint8)  # Restore the values from 0-1 to 0-255 (uint8 data type)\n",
        "  return img"
      ],
      "metadata": {
        "id": "QeqbRg8WW2k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img1 = preprocess_img(img1, angio=True)\n",
        "img2 = preprocess_img(img2)\n",
        "img3 = preprocess_img(img3, fundus=True)\n",
        "\n",
        "# Shapes\n",
        "print(\"img1 shape: \", img1.shape)\n",
        "print(\"img2 shape: \", img2.shape)\n",
        "print(\"img3 shape: \", img3.shape)"
      ],
      "metadata": {
        "id": "wSdkTDZwXhpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 - Visualize images"
      ],
      "metadata": {
        "id": "j5YE_zaXY44o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "for i, img in enumerate([img1, img2, img3]):\n",
        "  plt.subplot(1,3,i+1)\n",
        "  plt.imshow(img,'gray')"
      ],
      "metadata": {
        "id": "GOvf5SdgX-yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 - Manual extraction of features"
      ],
      "metadata": {
        "id": "uhMAdCcsZfbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are going to extract features from these three images. For feature extraction, you can use any numerical operation on arrays (images) you consider. Additionally, you can use [feature module](https://scikit-image.org/docs/dev/api/skimage.feature.html) from skimage package. Remember to import the feature module, everything will be easier!"
      ],
      "metadata": {
        "id": "6pfVlpk9ZyNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Complete the following lines to perform a feature extraction on the given images. It is up to you which and how many features you choose for this exercise."
      ],
      "metadata": {
        "id": "TECey3egasCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: import the feature module from skimage package\n",
        "\n",
        "\n",
        "# image 1\n",
        "feat1_img1 =\n",
        "feat2_img1 =\n",
        "featN_img1 =\n",
        "# image 2\n",
        "feat1_img2 =\n",
        "feat2_img2 =\n",
        "featN_img2 =\n",
        "# image 3\n",
        "feat1_img3 =\n",
        "feat2_img3 =\n",
        "featN_img3 ="
      ],
      "metadata": {
        "id": "JOoFpV32XrOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 - Visualize features extracted"
      ],
      "metadata": {
        "id": "QQOAGoJ4cJ19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# image\n",
        "print(\"feat1_img1 shape: \", feat1_img1.shape)\n",
        "print(\"feat2_img1 shape: \", feat2_img1.shape)\n",
        "print(\"featN_img1 shape: \", featN_img1.shape)\n",
        "print(\"feat1_img2 shape: \", feat1_img2.shape)\n",
        "print(\"feat2_img2 shape: \", feat2_img2.shape)\n",
        "print(\"featN_img2 shape: \", featN_img2.shape)\n",
        "print(\"feat1_img3 shape: \", feat1_img3.shape)\n",
        "print(\"feat2_img3 shape: \", feat2_img3.shape)\n",
        "print(\"featN_img3 shape: \", featN_img3.shape)"
      ],
      "metadata": {
        "id": "UizqnxvjbuIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In case you performed Local Binary Pattern or other operation which yielded a 256x256 output,\n",
        "# uncomment the line below to visualize the feature map extracted (adjust the variable in which\n",
        "# you have stored the result to visualize correctly)\n",
        "# plt.imshow(feat1_img1)"
      ],
      "metadata": {
        "id": "DkQoDrBo-l0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 3 - Playing aroung with a Multilayer Perceptron (MLP) and training parameters"
      ],
      "metadata": {
        "id": "3KO0Q0XxkAPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this final part, we are going to play around with a simple MLP through TensorFlow's playground:\n",
        "\n",
        "https://playground.tensorflow.org/\n",
        "\n",
        "Here, you will be able to play with an MLP by adding layers, neurons, modifying the activation functions applied after each layer, explore how different regularization techniques impact on the training, explore how the training changes with the learning rate, etc.\n",
        "\n",
        "For this task, answer the following questions:"
      ],
      "metadata": {
        "id": "PlGskJ-KuvIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "**Q1. For a classification problem, find an optimal configuration that allows your model to converge. Explore different configurations and detail what is the optimal settings achieved. You can lose the test loss as a metric to decide which configuration was optimal. Attach a screenshot of your set-up and result, AND detail the configurations employed (input data settings, number of layers, neurons per layer, learning rate, activation function, regularization, etc.).**"
      ],
      "metadata": {
        "id": "LkT0cx1nvpGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write here your answer"
      ],
      "metadata": {
        "id": "RVjlpDtDw6JM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Q2. For a classification problem, find a configuration that prevents your model from converging. Explore different configurations if needed. Attach a screenshot of your set-up and result, AND detail the configurations employed (input data settings, number of layers, neurons per layer, learning rate, activation function, regularization, etc.). Why is the model not converging with your configuration?**"
      ],
      "metadata": {
        "id": "pbYKRmjrxT8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write here your answer"
      ],
      "metadata": {
        "id": "BRgv6FMA_P0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [RECOMMENDED] Useful Resources"
      ],
      "metadata": {
        "id": "ffL-F9Eekfzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, please find below some useful resources that may be of interest to you in order to expand your knowledge about ML & DL:\n",
        "1. Visual intuition of CNN layers:\n",
        "  - Convolutions: https://deeplizard.com/resource/pavq7noze2\n",
        "  - MaxPooling: https://deeplizard.com/resource/pavq7noze3\n",
        "  - Transposed Convolution: https://deeplizard.com/resource/pavq7noze4\n",
        "  - Upsampling: https://deeplizard.com/resource/pavq7noze5\n",
        "2. ConvNet playground (activate adv. options): https://convnetplayground.fastforwardlabs.com/#/\n",
        "3. CNN Explainer: https://poloclub.github.io/cnn-explainer/\n"
      ],
      "metadata": {
        "id": "_BEKl5pDklHy"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pcLKNDYxH0EA",
        "0o8DJpIrJi-3",
        "uYthnGhRJwC3",
        "iKxL9XUkLUoe",
        "C7oOTsHfMCgq",
        "fCjR5C2pOUdl",
        "Htes449IPIxN",
        "qIX_n0_JQz80",
        "IA-KA9ECQ2jl",
        "Ck8VOySeR7xr",
        "skn-WikqWxVx",
        "e425sDr7W1K5",
        "j5YE_zaXY44o",
        "uhMAdCcsZfbv"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "0ad57050c77180dc9ed5ccc7774a474d285089782a3b5193155c6c81d567ba30"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}